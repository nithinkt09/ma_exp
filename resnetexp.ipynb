{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMHYANlXf0El2+S3Gi+6ZJH"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "eb2c232be9ac4dff950be272b685b9ec": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6a21e918140043168a3cbd2202944a16",
              "IPY_MODEL_77237e10340641419be77d2c73aa3906",
              "IPY_MODEL_304c39ce925743f58741567aa5c16fc8"
            ],
            "layout": "IPY_MODEL_33c46ec9455648d996d08d4aaf0df216"
          }
        },
        "6a21e918140043168a3cbd2202944a16": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_23a8fff57dbc4bdb8654a7b4b83cdee4",
            "placeholder": "​",
            "style": "IPY_MODEL_acf95b1e25d84dd1aed662c5b72fc684",
            "value": "100%"
          }
        },
        "77237e10340641419be77d2c73aa3906": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a1581166eb8b4f1ba1a00ca3b3c416a7",
            "max": 10,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_5e0f4416bd2b444ca52670c0e71740ab",
            "value": 10
          }
        },
        "304c39ce925743f58741567aa5c16fc8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4e46f10e864e4db5a281236ca8b00dee",
            "placeholder": "​",
            "style": "IPY_MODEL_0311d8e2db294b889b8c4ddf646f0900",
            "value": " 10/10 [08:21&lt;00:00, 50.04s/it]"
          }
        },
        "33c46ec9455648d996d08d4aaf0df216": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "23a8fff57dbc4bdb8654a7b4b83cdee4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "acf95b1e25d84dd1aed662c5b72fc684": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a1581166eb8b4f1ba1a00ca3b3c416a7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5e0f4416bd2b444ca52670c0e71740ab": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "4e46f10e864e4db5a281236ca8b00dee": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0311d8e2db294b889b8c4ddf646f0900": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bdiKbpCKpNGF",
        "outputId": "e97b6fd4-0226-4ded-97bc-84470c0ba745"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torchmetrics\n",
            "  Downloading torchmetrics-1.7.0-py3-none-any.whl.metadata (21 kB)\n",
            "Requirement already satisfied: numpy>1.20.0 in /usr/local/lib/python3.11/dist-packages (from torchmetrics) (2.0.2)\n",
            "Requirement already satisfied: packaging>17.1 in /usr/local/lib/python3.11/dist-packages (from torchmetrics) (24.2)\n",
            "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from torchmetrics) (2.6.0+cu124)\n",
            "Collecting lightning-utilities>=0.8.0 (from torchmetrics)\n",
            "  Downloading lightning_utilities-0.14.3-py3-none-any.whl.metadata (5.6 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (75.2.0)\n",
            "Requirement already satisfied: typing_extensions in /usr/local/lib/python3.11/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (4.13.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (3.18.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=2.0.0->torchmetrics)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=2.0.0->torchmetrics)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=2.0.0->torchmetrics)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=2.0.0->torchmetrics)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=2.0.0->torchmetrics)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=2.0.0->torchmetrics)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=2.0.0->torchmetrics)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=2.0.0->torchmetrics)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=2.0.0->torchmetrics)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=2.0.0->torchmetrics)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.0.0->torchmetrics) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.0.0->torchmetrics) (3.0.2)\n",
            "Downloading torchmetrics-1.7.0-py3-none-any.whl (960 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m960.9/960.9 kB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lightning_utilities-0.14.3-py3-none-any.whl (28 kB)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m64.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m48.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m30.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m61.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, lightning-utilities, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torchmetrics\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed lightning-utilities-0.14.3 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 torchmetrics-1.7.0\n"
          ]
        }
      ],
      "source": [
        "!pip install torchmetrics"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "import torchvision\n",
        "import torchvision.datasets as torch_datasets\n",
        "\n",
        "from torchvision.transforms import transforms\n",
        "from torch.utils.data import DataLoader, Dataset, SubsetRandomSampler\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "import math\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "from torchmetrics import Accuracy\n",
        "\n",
        "from timeit import default_timer as timer\n",
        "\n"
      ],
      "metadata": {
        "id": "g4qFzIehqe1V"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        " H(x) be stacked layers of Convolutions and BatchNorms, when these layers are stacked,\n",
        "instead of learning this layers, which are stacked, which could be computationally expensive and also might lead to issues of vanishing or exploding gradients,\n",
        "one method could be learn the underlying mapping of this stack or block; in this case, say if x is the input and H(x) is the output, the we could try to focus on the residual F(x), which is equal to H(x) - x.\n",
        "And now, if H(x) outputs a value that is approximately close to x, meaning they are approximately equal then F(x) would be approximately close to 0 or if H(x)\n",
        "despite having complex convolutional operations if it outputs some linear approximation of the input x, rather than trying to have the complicated stacked computations of H(x)\n",
        "or even if it has some non-linear approximations, when there are deep layers, F(x) could still be proven to offer better way to handle,\n",
        "where it captures a potential linear or non-linear mapping that has an optimal approximation, not if the exact an optimal approximation.\n",
        "Where, if this method is carried out in multiple blocks, where in each block the function or mapping is trying to get F(x) r\n",
        "ather than deep or complex notions of stacked layers of H(x), it would also help in capturing the global and most generalized representation of the mapping,\n",
        "which not only addresses the issues of over-fitting but also potentially prevents from degradation.\n",
        "################################################################################\n",
        "\n",
        "\n",
        "residual_block:\n",
        "    - goal: the goal of resnet is to find an optimal residual function that captures the underlying pattern\n",
        "    - let this block with stacks of layers represent H(x), where x is the input and H(x) is a combined stack layer or composition of convolutions and batchnorm\n",
        "    - the stack of layers or H(x) and it's input x can be uesd together to find a residual function F(x), which captures the underlying mapping or computations of the stack\n",
        "                which could be represented as F(x) = H(x) -x, where here F(x) could be considered as an approximate representation of the underlying computtations of H applied on x\n",
        "\n",
        "basic_residual_block: (num_in_channels, num_out_channels, stride)\n",
        "    -in_channels: represents number of input_channels that this block takes\n",
        "    -out_channels: represents number of output_channels that this block outputs\n",
        "    - stride: stride of the filters that traverses using convolutions across the image\n",
        "\n",
        "    if this particular block has two stacked layers, then they could be represented as follows:\n",
        "\n",
        "        # assuming that the given stride is 1, however, changing stride could change the dimensions\n",
        "        # block1:  this block changes the channel size as required,\n",
        "        with filter size as per standard approach is taken as 3, and with padding and stride of 1, this layer doesn't change the spatial dimension but only the channel depth\n",
        "\n",
        "        conv1 = (input_channels, output_channels, filter=3, stride=stride, padding=1)\n",
        "        batchnorm(output_channels), # normalizes the values across the volume of the output from conv1\n",
        "        --------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "        # block2: this block also updates the channel size as required and following similar convention of block 1\n",
        "        the output_channels from the previous block becomes the input_channels to this conv2 block,\n",
        "        this block also ensures that by hardcoding stride as 1, it ensures that the height and width remains the same as the output block\n",
        "\n",
        "        conv2 = (output_channels, output_channels, filter=3, stride=1, padding=1)\n",
        "        batchnorm(output_channels), # normalizes the values across the volume of the output from conv1\n",
        "        --------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "        for the residual function F(x) = H(x) - x to work, they all must have same (channel_count, height, width)\n",
        "\n",
        "        here, since H(x) has already learnt the weights that correctly map the input and output,\n",
        "        rather than changing the dimension of the output of H(x) to match the input, an optimal way would be\n",
        "        to project the input so that it matches the output dimension H(x) and also offering efficient way to comput F(x).\n",
        "\n",
        "         basically, for the residual F(x) to work, the output of H(x) must have the same (channel_count, height, width) of the input x\n",
        "\n",
        "        for example: if the input is (3, 224, 224)  and the output of H(x) is (64, 112, 112), then for F(x) to be able to learn the mapping,\n",
        "        the input x must be projected in such a way that it has the same dimensions of H(x),\n",
        "        where F(x) = H(x) - x, x is now projected into same spatial dimension as H(x)\n",
        "        ------------------------------------------------------------------------------------------------------------------\n",
        "        let x be an input of shape (3, 224, 224) and it goes through H(x) stack in the following manner:\n",
        "        ### case1: stride = 1, padding =1, kernel = 3\n",
        "        conv_1: input(3, 224, 224) --(in_channels=3, out_channels=64, kernel=3, stride=1, padding=1) --> output(64, 224, 224)\n",
        "        batchnorm(64)\n",
        "\n",
        "        # second convolution block has hardcoded stride and padding values of 1 with kernel size 3 so it outputs the same dimension as input\n",
        "        conv_2: input(64, 224, 224) --(in_channels=3, out_channels=64, kernel=3, stride=1, padding=1)--> output(64, 224, 224)\n",
        "        batchnorm(64)\n",
        "\n",
        "        result: however, here shape of channels is not the same despite having retiaining the dimensions of height and width, which must  be adjusted\n",
        "        ------------------------------------------------------------------------------------------------------------------\n",
        "        ### case2: if stride != 1 assume it to be 2, then it reduces the output size by downsampling the dimensions\n",
        "        conv_1: input(3, 224, 224) --(in_channels=3, out_channels=64, kernel=3, stride=2, padding=1) --> output(64, 112, 112)\n",
        "        batchnorm(64)\n",
        "\n",
        "        # second convolution block has hardcoded stride and padding values of 1 with kernel size 3 so it outputs the same dimension as input\n",
        "        conv_2: input(64, 112, 112) --(in_channels=3, out_channels=64, kernel=3, stride=1, padding=1)--> output(64, 112, 112)\n",
        "        batchnorm(64)\n",
        "\n",
        "        result: here, shape of both height and width and channels are mismath, which must be adjusted\n",
        "        ------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "        ### case3: if stride == 1 and input_channels== output_channels = 64\n",
        "        conv_1: input(64, 112, 112) --(in_channels=3, out_channels=64, kernel=3, stride=1, padding=1) --> output(64, 112, 112)\n",
        "        batchnorm(64)\n",
        "\n",
        "        # second convolution block has hardcoded stride and padding values of 1 with kernel size 3 so it outputs the same dimension as input\n",
        "        conv_2: input(64, 112, 112) --(in_channels=3, out_channels=64, kernel=3, stride=1, padding=1)--> output(64, 112, 112)\n",
        "        batchnorm(64)\n",
        "\n",
        "        result: here, shape of both height and width and also color channels remain the same, so there is no need for adjustment\n",
        "\n",
        "        shape(H(x)) == shape(x) = (64, 112, 112), then F(x) is also of shape (64, 112, 112) and F(x) = H(x) - x\n",
        "                H(x) = F(x) + x; where now the model's goal is to learn and approximate F(x)\n",
        "        ------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "        considering case 1 and  2 with mismatched dimensions, they can be adjusted as follows:\n",
        "        here, x must be adjusted by projecting it to have the same spatial dimension of H(x)\n",
        "\n",
        "        for example, H(x),shape = (64, 112, 112) and x.shape = (3, 224, 224)\n",
        "        adjusted_x = input(3, 224, 224) --(in_channels=3, out_channels=64, stride=2, padding=1)--> (64. 112. 112)\n",
        "        in the above operation the stride must be equal to the same stride that was used in the convolution operation\n",
        "        since the second convolution operation does not change or alter the dimension\n",
        "\n",
        "        once done, H(x) and x have the same dimension, where the goal would be then to learn the residual F(x) + x\n",
        "        which captrues the linear or non-linear transformation that was captrured by stacked layers in H(x)\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import math\n",
        "class ResidualBasicBlock(nn.Module):\n",
        "\n",
        "    \"\"\"\n",
        "    ResidualBasicBlock: to compute the residual, a learnable from stacks of layers, it may not capture the exact computation of the stacks of layers,\n",
        "    however, it could help in capturing a more generalized approximation, which could be more helpful, especially with diverse datasets\n",
        "\n",
        "    for a given input data x if the stacks of convolution and batchnorm operations are represented by H(x),\n",
        "    the residual could be represented by F(x), where F(x) = H(x) - x => H(x) = F(x) + x\n",
        "    \"\"\"\n",
        "\n",
        "    \"\"\"\n",
        "    in the following code, just as a representation, input_channels are assumed to be (3, 224, 224)\n",
        "    bias is set to false as batchnormalization scales and adjusts the values, where using additional learning parameter of bias would become redundant\n",
        "    and also adds computational costs, where the bias even if added would become negligble as batchnorm already scales and adjusts in an optimal manner,\n",
        "    additionaly batchnorm's beta values in the equation y * (conv(x) - mean)/stand_dev + beta;\n",
        "    if not exactly the same, but approximately captures/includes bias value which could also be optimal for more generalized predictions\n",
        "    output_size = floor(((image_size_1 + 2*padding - filter)/stride) + 1)\n",
        "    \"\"\"\n",
        "    def __init__(self, input_channels, output_channels, stride=1):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            input_channels: number of  channels in the input passed into the block\n",
        "            output_channels: number of output channels of this block\n",
        "            stride: stride of the filter that is used to traverse through the image, is only used in the first convolution layer\n",
        "                    where if the stride is other than 1 then it is used again in the projecttion matrix to readjust the projected_x to match the dimension of the output\n",
        "        \"\"\"\n",
        "        super(ResidualBasicBlock, self).__init__()\n",
        "\n",
        "        # below coded inputs and outputs are just a conceptualization and not the exact representation, assuming height and width are of same dimension image_size_\n",
        "\n",
        "        # if in_channels = 3 and image_size=224 and output_channels=64\n",
        "        # stride=1: input(3, 224, 224) -> output(64, 224, 224)\n",
        "        # stride=2: input(3, 224, 224) -> output(64, 112, 112), in which case x is projected to match the size of the output of this stack H(x)\n",
        "\n",
        "        # in this first layer conv_1, stride can be adjusted\n",
        "        self.conv_1 = nn.Conv2d(in_channels=input_channels, out_channels=output_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.batchnorm_1 = nn.BatchNorm2d(output_channels)\n",
        "\n",
        "        # in this second layer conv_2, stride is hardcoded to be 1 so it doesn't downsample the output of conv_1\n",
        "        # input(64, 224, 224) -> output(64, 224, 224) or input(64, 112, 112) -> output(64, 112, 112)\n",
        "        self.conv_2 = nn.Conv2d(in_channels=output_channels, out_channels=output_channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.batchnorm_2 = nn.BatchNorm2d(output_channels)\n",
        "\n",
        "\n",
        "        \"\"\"\n",
        "        in the conv_1 layer, if stride != 1, the input and output dimensions of H(x) do not match, thus x must be projected to match the dimension of H(x)\n",
        "        if in_channels != out_channels, then x must be projected to match the dimension of H(x),\n",
        "        since the goal is to get F(x) = H(x) -x => H(x) = F(x) -x\n",
        "        since the conv_2 layer doesn't change the spatial dimension, there is no need of checking or having any additional computations for that\n",
        "\n",
        "        both the channel and height or width could be adjusted in the following manner,\n",
        "        however, if stride == 1 and input_channels == output_channels, then x could be just sent as is without any update\n",
        "        these representation is generally referred to as shortcut, so the same name is used here to keep it  consistent\n",
        "        \"\"\"\n",
        "        self.shortcut = nn.Sequential() # when an input tensor is passed into this it returns the same without any changes => shortcut(x) == x\n",
        "\n",
        "        if (stride != 1) or input_channels != output_channels:\n",
        "            \"\"\"\n",
        "            since stride values are only dynamically added to conv_1 layer, the same stride value can be used here to project x to match the output dimension of H(x)\n",
        "            and since there was one such operation, x can also be projected only once to match it\n",
        "            \"\"\"\n",
        "            self.shortcut = nn.Sequential(\n",
        "                                                            nn.Conv2d(in_channels=input_channels, out_channels=output_channels, kernel_size=1, stride=stride, padding=0), #x: input(3, 224, 224) -> projected_x: output(64, 112, 112)\n",
        "                                                            nn.BatchNorm2d(output_channels)\n",
        "            )\n",
        "\n",
        "    # forward propagation\n",
        "    def forward(self, x):\n",
        "\n",
        "        # if stride != 1 or if input_channels != output_channels then the input is projected to match the output dimension and if not it outputs the input x without any transformation\n",
        "        identity = self.shortcut(x)\n",
        "        x = self.conv_1(x)\n",
        "        x = self.batchnorm_1(x)\n",
        "        x = self.conv_2(x)\n",
        "        x = self.batchnorm_2(x)\n",
        "        x += identity # H(x) = F(x) + x\n",
        "        x = F.relu(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "\"\"\"\n",
        "in the residual network, a stack of layers could be represented by H(x), and if x is the input,\n",
        "one of the ways to capture the computation is by capturing the change of the stacked layers represented by H(x), where if x is the input,\n",
        "then such change could be mapped by a function F(x) = H(x) - x, where if x and H(x) have the same dimension then x could be used as is,\n",
        "however, if H(x) and x do not have the same dimension then x is projected so that it matches the dimension of H(x),\n",
        "where the goal would be to approximate the F(x) + x that could be close to H(x) where it need be exactly as H(x),\n",
        "but approximately close to H(x), where such approximation might be missing the exact mapping,\n",
        " it could also be considered beneficial as it is a more generalized representation of the network,\n",
        "  which could be beneficial in networks with deeper layers and also in data which could be diverse.\n",
        "\"\"\"\n",
        "# resnet architecture\n",
        "\n",
        "class ResNetBasic(nn.Module):\n",
        "    \"\"\"\n",
        "    input image (tensor representation) must be resized to 224, 224\n",
        "    however, if the input_size is anything other than 224, then the adapativemaxpool adjusts so that first block always outputs 56\n",
        "    \"\"\"\n",
        "    def __init__(self, num_classes=10):\n",
        "\n",
        "        super(ResNetBasic, self).__init__()\n",
        "\n",
        "        self.conv_1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False) # input(3, 224, 224) - > output(64, 112, 112)\n",
        "        self.batchnorm_1 = nn.BatchNorm2d(64)\n",
        "        self.adaptive_maxpool_1 = nn.AdaptiveMaxPool2d(56) # input (64, 112, 112) -> output(64, 56, 56)\n",
        "\n",
        "        self.stage_1 = self._make_stack(64, 64, stride=1, num_blocks=2) #input (64, 56, 56) -> output (64, 56, 56)\n",
        "        self.stage_2 = self._make_stack(64, 128, stride=2, num_blocks=2) # input(64, 56, 56) -> output(128, 28, 28)\n",
        "        self.stage_3 = self._make_stack(128, 256, stride=2, num_blocks=2) # input(128, 28, 28) -> output(256, 14, 14)\n",
        "        self.stage_4 = self._make_stack(256, 512, stride=2, num_blocks=2) # input(256, 14, 14) -> output(512, 7, 7)\n",
        "\n",
        "        self.adaptive_avgpool = nn.AdaptiveAvgPool2d((1, 1)) # input(512, 7, 7) -> output(512, 1, 1)\n",
        "        self.fully_connected = nn.Linear(512, num_classes)\n",
        "\n",
        "        self.flatten = nn.Flatten(start_dim=1) # considering that 0th dim is batch_number\n",
        "\n",
        "    def _make_stack(self, input_channels, output_channels, stride, num_blocks):\n",
        "        \"\"\"\n",
        "        this method uses the residual block class defined above to make stack of layers with x as input:\n",
        "        conv->batchnorm->conv->batchnorm-> = y\n",
        "        x_projected = project(x) to match the dimension of y\n",
        "        output = relu(x_projected + y)\n",
        "        \"\"\"\n",
        "        layers = []\n",
        "        layers.append(ResidualBasicBlock(input_channels, output_channels, stride)) # may downsample if stride > 1\n",
        "\n",
        "        for _ in range(1, num_blocks):\n",
        "            #in this loop, residual_basic_blocks are added without changing the stride or output channels\n",
        "            layers.append(ResidualBasicBlock(output_channels, output_channels, stride=1))\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    # forward propagation\n",
        "    def forward(self, x):\n",
        "        x = self.conv_1(x)\n",
        "        x = self.batchnorm_1(x)\n",
        "        x = self.adaptive_maxpool_1(x)\n",
        "        x = self.stage_1(x)\n",
        "        x = self.stage_2(x)\n",
        "        x = self.stage_3(x)\n",
        "        x = self.stage_4(x)\n",
        "        x = self.adaptive_avgpool(x)\n",
        "        x = self.flatten(x)\n",
        "        x = self.fully_connected(x)\n",
        "        return x\n",
        "\n"
      ],
      "metadata": {
        "id": "6WV_Wb0-rFLy"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "RANDOM_SEED = 32\n",
        "BATCH_SIZE = 64\n",
        "MODEL_DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "torch.manual_seed(RANDOM_SEED)\n",
        "\n",
        "accuracy_metrics = Accuracy(task=\"multiclass\", num_classes=10).to(MODEL_DEVICE)"
      ],
      "metadata": {
        "id": "1QOW1Krht0oo"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "NUM_CPUS = os.cpu_count()\n",
        "\n",
        "# normalized mean and standard deviation values that are used in transforming data, set per CIFAR10 datset\n",
        "NORMALIZED_MEAN=[0.4914, 0.4822, 0.4465]\n",
        "NORMALIZED_STD = [0.2023, 0.1994, 0.2010]\n",
        "\n",
        "torch.manual_seed(RANDOM_SEED)     # manual seed to ensure that the data can be reproducible\n",
        "\n",
        "def cifar10data(\n",
        "        data_dir = '/data/cifar10',\n",
        "        batch_size=32,\n",
        "        image_size=32,\n",
        "        test=False,\n",
        "        eval_size=0.1,\n",
        "        normalized_mean=[0.4914, 0.4822, 0.4465],\n",
        "        normalized_std=[0.2023, 0.1994, 0.2010],\n",
        "        num_cpus = NUM_CPUS):\n",
        "    \"\"\"\n",
        "    loads cifar10 dataset and returns dataloaders\n",
        "    Args:\n",
        "        - datadir (str): root directory or the folder to where the data must exist\n",
        "        - image_size (int): adjusts the size of the image from the datasets to (image_size, image_size), ex: (224, 224)\n",
        "        - train (bool): if True, will use the train split of the dataset\n",
        "        - eval_size(float): size of the validation dataset from the training dataset, if the eval_size is 0.1, then the valid_dataset size would be 10% of the training dataset\n",
        "        - batch_size= size of batch that DataLoader process  (number of samples per batch)\n",
        "        - normalized_mean: values used to normalize the mean of tensor representation of data\n",
        "        - normalized_std: values used to normalized the standard deviation of tensor representation of data\n",
        "    Return:\n",
        "        - if train is set to false, will return test DataLoader\n",
        "        - if train is set to true, splits cifar10 dataset into train and test data and returns respective dataloaders along with class labels\n",
        "    \"\"\"\n",
        "\n",
        "    # resize the data to the give image_size, transform to tensors, and normalize mean and standard deviation\n",
        "    transform_data = transforms.Compose([\n",
        "        transforms.Resize((image_size, image_size)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=normalized_mean, std=normalized_std)\n",
        "    ])\n",
        "\n",
        "    if test:\n",
        "        test_dataset = torch_datasets.CIFAR10(root=data_dir, train=False, transform=transform_data, download=True)\n",
        "        dataloader = DataLoader(dataset=test_dataset, batch_size=batch_size, num_workers=num_cpus)\n",
        "        return dataloader\n",
        "    else:\n",
        "        \"\"\"\n",
        "        training dataset is split into train and evalation sections using randomsampler, and also shuffle is turned off as randomsampler already samples from random indicies without a strict order as defined in the dataset\n",
        "        \"\"\"\n",
        "        train_dataset = torch_datasets.CIFAR10(root=data_dir, train=True, transform=transform_data, download=True)\n",
        "        len_train_dataset = len(train_dataset)  # gets the length of the train_dataset which are then converted to indicies\n",
        "        indicies = list(range(len_train_dataset))\n",
        "        split = int(len_train_dataset*eval_size) # number that splits the training data into train and evaluation sections\n",
        "        train_idx, eval_idx = indicies[:split], indicies[split:] # splits the indicies that are used in randomsampler method to get random indicies of the respective split size\n",
        "\n",
        "        train_sampler = SubsetRandomSampler(train_idx)\n",
        "        eval_sampler = SubsetRandomSampler(eval_idx)\n",
        "\n",
        "        # train and eval dataloaders that uses randomsampler to split it respectively\n",
        "        train_dataloader = DataLoader(train_dataset, batch_size, sampler=train_sampler, num_workers=num_cpus)\n",
        "        eval_dataloader = DataLoader(train_dataset, batch_size, sampler=eval_sampler, num_workers=num_cpus)\n",
        "\n",
        "\n",
        "        image, label = next(iter(train_dataloader))\n",
        "        data_attributes = {\n",
        "            #\"class_labels\": train_dataset.classes,\n",
        "            \"classes_to_index\": train_dataset.class_to_idx,\n",
        "            \"image/tensor shape\": image[0].shape,\n",
        "            \"dataloader_shape\": image.shape,\n",
        "            \"len_train_data(dataloader)\": len(train_dataloader) * batch_size,\n",
        "            \"len_evaal_data(dataloader)\": len(eval_dataloader) * batch_size,\n",
        "            \"classes\\target_labels\": len(train_dataset.classes)\n",
        "        }\n",
        "\n",
        "        return train_dataloader, eval_dataloader, data_attributes"
      ],
      "metadata": {
        "id": "bTRs8byKr7C7"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cifar10_train_data, cifar10_eval_data, cifar_data_attributes = cifar10data()\n",
        "cifar10_test_data = cifar10data(test=True)\n",
        "cifar_data_attributes"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DDSFWu5uvaac",
        "outputId": "b8eb306b-ccd2-49df-bc19-36b003b3f3cf"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170M/170M [00:02<00:00, 70.2MB/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'classes_to_index': {'airplane': 0,\n",
              "  'automobile': 1,\n",
              "  'bird': 2,\n",
              "  'cat': 3,\n",
              "  'deer': 4,\n",
              "  'dog': 5,\n",
              "  'frog': 6,\n",
              "  'horse': 7,\n",
              "  'ship': 8,\n",
              "  'truck': 9},\n",
              " 'image/tensor shape': torch.Size([3, 32, 32]),\n",
              " 'dataloader_shape': torch.Size([32, 3, 32, 32]),\n",
              " 'len_train_data(dataloader)': 5024,\n",
              " 'len_evaal_data(dataloader)': 45024,\n",
              " 'classes\\target_labels': 10}"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# initialize the ResNetBasic network\n",
        "res_model_v0 = ResNetBasic().to(MODEL_DEVICE)\n",
        "\n",
        "# optimizer and loss functions\n",
        "optimizer = optim.Adam(params=res_model_v0.parameters())\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "\n",
        "NUM_EPOCHS = 10 # number of training loops"
      ],
      "metadata": {
        "id": "7d2YowT7rUpr"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(RANDOM_SEED)     # manual seed to ensure that the data can be reproducible\n",
        "\n",
        "start_time= timer()\n",
        "for epoch in tqdm(range(NUM_EPOCHS)):\n",
        "    print(f\"epoch:{epoch}______________\")\n",
        "\n",
        "    accuracy_metrics.reset() # reset the accuracy metrics to prevent accumulation\n",
        "\n",
        "    # metrics for training\n",
        "    train_loss_list = [] # append the training loss per batch, keeps track of individual losses, it could be summed up and divided to get the averge training loss per batch\n",
        "    eval_loss_list = [] # similar to training loss, but used in evaluation loss which is done in evaluation method\n",
        "    eval_accuracy_list = [] # evaluate predictions against the truth labels and append to the list\n",
        "\n",
        "    # training loop\n",
        "    for batch, (images, labels) in enumerate(cifar10_train_data): # traverse/iterates over each batch\n",
        "        images, labels = images.to(MODEL_DEVICE), labels.to(MODEL_DEVICE) #move data to MODEL_DEVICE which is used both on model and also in evaluation\n",
        "\n",
        "        res_model_v0.train() # set the model to train mode\n",
        "        optimizer.zero_grad() # reset the optimizers to prevent gradient accumulation\n",
        "\n",
        "        train_logits = res_model_v0(images) # model outputs logits as softmax is not applied in the model\n",
        "        train_loss = criterion(train_logits, labels) # calculate the loss between predicted logits and the actual truth values of target\n",
        "\n",
        "        train_loss_list.append(train_loss.item()) # append the loss to the list of loss\n",
        "\n",
        "        train_loss.backward() # back propagation\n",
        "        optimizer.step() # updates the weights and other learnable parameters (this training model is used for resnet, so it tries to learn F(x) + x or approximate values or the outputs of H(x))\n",
        "\n",
        "\n",
        "    # evaluate the model at the end of each epoch to check the training and also evaluation\n",
        "    res_model_v0.eval() # sets the model to evaluation mode\n",
        "    for eval_batch, (eval_images, eval_labels) in enumerate(cifar10_eval_data): # traverses/iterates over each batch\n",
        "        eval_images, eval_labels = eval_images.to(MODEL_DEVICE), eval_labels.to(MODEL_DEVICE)\n",
        "\n",
        "        with torch.inference_mode(): #  disables gradient_descent / learning\n",
        "            eval_logits = res_model_v0(eval_images)\n",
        "            eval_loss = criterion(eval_logits, eval_labels)\n",
        "            eval_loss_list.append(eval_loss.item())\n",
        "\n",
        "            #torch.softmax(batch_size, num_classes) when dim=1, it takes probabilities for each class in the row\n",
        "            # torch.argmax when dim=1 : takes the class index with highest probabilitiy for each sample\n",
        "            eval_predictions = torch.argmax(torch.softmax(eval_logits, dim=1), dim=1)\n",
        "            eval_accuracy = accuracy_metrics(eval_predictions, eval_labels).item() # evaluates the accuracy with true_positives/(true_positivies + false_positivies)\n",
        "            eval_accuracy_list.append(eval_accuracy)\n",
        "\n",
        "    # average out and print loss and accuracy\n",
        "    avg_train_loss = sum(train_loss_list) / len(train_loss_list)\n",
        "    avg_eval_loss = sum(eval_loss_list) / len(eval_loss_list)\n",
        "    avg_eval_accuracy = sum(eval_accuracy_list) / len(eval_accuracy_list)\n",
        "    print(f\"avg_train_loss: {avg_train_loss} | avg_eval_loss:{avg_eval_loss} | avg_eval_accuracy:{avg_eval_accuracy}\")\n",
        "\n",
        "end_time = timer()\n",
        "\n",
        "print(f\"total_train_time: {end_time-start_time}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424,
          "referenced_widgets": [
            "eb2c232be9ac4dff950be272b685b9ec",
            "6a21e918140043168a3cbd2202944a16",
            "77237e10340641419be77d2c73aa3906",
            "304c39ce925743f58741567aa5c16fc8",
            "33c46ec9455648d996d08d4aaf0df216",
            "23a8fff57dbc4bdb8654a7b4b83cdee4",
            "acf95b1e25d84dd1aed662c5b72fc684",
            "a1581166eb8b4f1ba1a00ca3b3c416a7",
            "5e0f4416bd2b444ca52670c0e71740ab",
            "4e46f10e864e4db5a281236ca8b00dee",
            "0311d8e2db294b889b8c4ddf646f0900"
          ]
        },
        "id": "9iHnoRR301lY",
        "outputId": "830826a2-e30c-4987-9efb-3fd74802344b"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/10 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "eb2c232be9ac4dff950be272b685b9ec"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch:0______________\n",
            "avg_train_loss: 2.004547108510497 | avg_eval_loss:2.1629204389976118 | avg_eval_accuracy:0.2564410092395167\n",
            "epoch:1______________\n",
            "avg_train_loss: 1.7953805065458748 | avg_eval_loss:1.8948389577289464 | avg_eval_accuracy:0.3013948116560057\n",
            "epoch:2______________\n",
            "avg_train_loss: 1.7090957627934256 | avg_eval_loss:1.7764679269749981 | avg_eval_accuracy:0.3659159559346126\n",
            "epoch:3______________\n",
            "avg_train_loss: 1.6088916169609992 | avg_eval_loss:1.7080095192177887 | avg_eval_accuracy:0.3747334754797441\n",
            "epoch:4______________\n",
            "avg_train_loss: 1.5406295729290909 | avg_eval_loss:1.865507679114379 | avg_eval_accuracy:0.371113184079602\n",
            "epoch:5______________\n",
            "avg_train_loss: 1.4823845829933313 | avg_eval_loss:1.65508154850101 | avg_eval_accuracy:0.3999200426439232\n",
            "epoch:6______________\n",
            "avg_train_loss: 1.419481732283428 | avg_eval_loss:1.5310461103450177 | avg_eval_accuracy:0.4330135039090263\n",
            "epoch:7______________\n",
            "avg_train_loss: 1.3482386716611825 | avg_eval_loss:1.4590825399101923 | avg_eval_accuracy:0.47752309879175553\n",
            "epoch:8______________\n",
            "avg_train_loss: 1.2853355054642743 | avg_eval_loss:1.600768761128166 | avg_eval_accuracy:0.42630597014925375\n",
            "epoch:9______________\n",
            "avg_train_loss: 1.2186361145062052 | avg_eval_loss:1.4517191011768413 | avg_eval_accuracy:0.48858386638237383\n",
            "total_train_time: 501.033035694\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "moel_save_path"
      ],
      "metadata": {
        "id": "QoNywLBHczab"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}